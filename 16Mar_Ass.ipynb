{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5878469b-fbdb-4f1c-a2df-a3e1b0582f4a",
   "metadata": {},
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee66db2-379d-4696-b44e-4d870f0dfe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "# can they be mitigated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c97b8eb-1bd6-42c7-816e-6052fe9814ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overfitting occurs when a model is too complex, fitting the training data too closely and performing poorly on new, unseen data.\n",
    "#Consequences include poor generalization and high variance.\n",
    "\n",
    "#Underfitting happens when a model is too simple to capture the underlying patterns, resulting in poor performance on both training and test data.\n",
    "\n",
    "#Mitigating overfitting involves reducing model complexity, using more data, or applying regularization techniques.\n",
    "#Addressing underfitting requires increasing model complexity, gathering more relevant features, or choosing a more sophisticated model. \n",
    "#Techniques like cross-validation and adjusting hyperparameters help strike a balance between overfitting and underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08e5ee7-2f26-41d4-bf5d-c55e5ad4e071",
   "metadata": {},
   "source": [
    "Q2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9daab309-11a1-466c-8558-db03af662e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aced5389-4c31-4547-8fc2-94d470c42a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To reduce overfitting in machine learning models, one can employ techniques such as:\n",
    "\n",
    "#1.Regularization: Penalize overly complex models.\n",
    "#2.Cross-validation: Assess model performance on different data subsets.\n",
    "#3.Pruning: Trim unnecessary branches in decision trees.\n",
    "#4.Feature selection: Use only relevant features.\n",
    "#5.Increasing data: Gather more diverse training examples.\n",
    "#6.Ensemble methods: Combine predictions from multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e689c1-3ce0-42fe-8d52-1c205fedb5f5",
   "metadata": {},
   "source": [
    "Q3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f3d86a5-0e2e-44e6-9820-3849d056a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e86ccfa1-7a02-4c6a-a004-9ba1f43d5e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Underfitting occurs when a machine learning model is too simple to capture the underlying \n",
    "#patterns in the data, leading to poor performance on both training and test sets. Scenarios\n",
    "#where underfitting may occur include insufficient model complexity, using too few features,\n",
    "#or employing an algorithm that is not sophisticated enough for the complexity of the data.\n",
    "#In these cases, the model fails to adequately learn the relationships within the dataset,\n",
    "#resulting in suboptimal predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff81f3e1-c1af-4cab-af20-e67ae2b1f302",
   "metadata": {},
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a4b5b0e-f8ed-4638-9c08-5d79a1cc9897",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "#variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d37120c6-a1f3-40c7-90be-2032ca172fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The bias-variance tradeoff in machine learning is the delicate balance between model \n",
    "#simplicity (high bias) and flexibility (high variance). Bias refers to errors introduced\n",
    "#by overly simplistic models, while variance represents sensitivity to variations in the training\n",
    "#data. High bias tends to underfit, and high variance leads to overfitting. Achieving an optimal\n",
    "#tradeoff minimizes overall errors. Balancing bias and variance enhances a model's ability to\n",
    "#generalize well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4375a426-359e-42cc-8a34-9099ef9c2906",
   "metadata": {},
   "source": [
    "Q5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45767fb2-0ece-4aca-9d34-21389b6ea7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83148a0b-c93d-45d2-b447-39fcf27c4dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common methods for detecting overfitting and underfitting include:\n",
    "\n",
    "#1. Cross-validation: Assessing model performance on different data subsets.\n",
    "#2. Learning curves: Analyzing training and validation performance over iterations.\n",
    "#3. Validation metrics: Monitoring metrics on a validation set.\n",
    "#4. Model evaluation: Examining performance on unseen data.\n",
    "#5. Visual inspection: Plotting actual vs. predicted values for patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2cf844-5028-4d59-a68a-bd0c3fa7b189",
   "metadata": {},
   "source": [
    "Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8af6cde-741d-452d-b24f-4dd2962e3199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "#and high variance models, and how do they differ in terms of their performance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b178258-1c62-4ab4-91af-34f3ba5b6f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bias and variance are components of the bias-variance tradeoff in machine learning. \n",
    "#High bias (underfitting) occurs when a model is too simple, failing to capture underlying \n",
    "#patterns. Examples include linear regression on a non-linear dataset. High variance (overfitting)\n",
    "#results from excessive model complexity, fitting noise along with patterns. Examples include \n",
    "#decision trees with deep branches. While high bias leads to systematic errors, high variance \n",
    "#leads to sensitivity to variations. Achieving a balance optimizes model performance by minimizing \n",
    "#both bias and variance, striking a compromise between simplicity and flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d2248-d027-4342-b64b-d56bbe5be783",
   "metadata": {},
   "source": [
    "Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f0759-c884-40a1-b14b-b68251b9e49c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
